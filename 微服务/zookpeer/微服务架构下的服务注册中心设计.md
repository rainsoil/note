date: 2019年11月8日10:49:12

分类: 

- zookeeper

tag:

-  zookeeper

---
#  微服务架构下的服务注册中心设计

这里以一个电商网站进行模拟

## 单体架构

传统的单体架构(all in one) 

![](http://files.luyanan.com//img/20191104093834.png)

## 分布式架构

假设一个电商的下单场景,完成一笔订单入库操作,需要做以下几个操作:

1. 创建订单
2. 卡券抵扣
3. 库存扣减

那么在分布式架构的调用链下, 可能是下面这种情况. 那么服务和服务之间势必会存在远程通信. 

![](http://files.luyanan.com//img/20191104094509.png)

为了让大家更好的理解服务之间的通信, 我们来基于SpringBoot 模拟出上面的这种应用场景.

### 使用SpringBoot + RestTemplate 创建两个服务

创建两个SpringBoot 工程

- 订单服务(order-service)
- 库存服务(repo-service)

分别创建 controller

 OrderController 

```java
@Autowired
RestTemplateBuilder restTemplateBuilder;

@PostMapping("/order")
public String sayHello(){
     RestTemplate rt=restTemplateBuilder.build();
     System.out.println("开始创建订单");
     rt.put("http://localhost:8081/repo/{1}",null,10001);
 return "SUCCESS";
}

```

 RepoController 

```java
@PutMapping("/repo/{pid}")
public void serverMsg(@PathVariable("pid") String pid){
    System.out.println("扣减库存,商品 ID:"+pid);
}

```

###  简单了解 RestTemplate

服务与服务之间, 一定不是相互隔离的, 而是必须要互相联系进行数据通信才能实现完整的功能. 所以在刚才的案例中, 我妈们拆分出来的服务使用RestTemplate 来进行远程通信. 

在了解RestTemplate 之前, 先来简单了解下HTTP Client, 我们实现对于http 服务的远程调用,常见的手段是基于Apache 提供的httpclient, 或者是Square 公司开源的Okhttp. 还有Netflix 提供的Feign等等.

简单来说,RestTemplate 是Spring 提供了用来访问REST 服务的客户端, 以前我们使用Apache HttpClient  来进行远程调用的时候, 需要些非常多的代码,还需要考虑各种资源回收的问题. 而RestTemplate 简化了Http 服务的通信, 我们只需要提供URL，RestTemplate 会帮我们搞定这一切. 

另外,需要注意的是, RestTemplate 并没有重复造轮子, 而是利用现有的技术,如JDK或者Apache HttpClient、Okhttp 等实现http 远程调用. 

#### 源码

RestTemplate 需要使用一个实现了 ClientHttpRequestFactory  接口的类为其提供 ClientHttpRequest  实现。 而  ClientHttpRequest  则实现封装了组装、发送HTTP消息以及解析响应的底层细节.

目前( 5.1.8.RELEASE) 的RestTemplate 主要有四种  ClientHttpRequestFactory  的实现, 他们分别是: 

1. 基于JDK HttpUrlConnection 的 SimpleClientHttpRequestFactory 
2. 基于Apache HttpComponents Client 的 HttpComponentsClientHttpRequestFactory 
3. 基于Okhttp2(Okhttp 最新版本为3, 有较大改动, 包名有变动, 不和老版本兼容)的  OkHttpClientHttpRequestFactory 
4. 基于Netty4 的 Netty4ClientHttpRequestFactory 

####  消息读取的转化

RestTemplate 对于服务端端返回消息的读取, 提供了消息转化器, 可以把目标消息转换为用户指定的格式( 通过 Class responseType 参数指定 ) 指定. 类似于写消息的处理,读消息的处理也是通过 ContentType 和 responseType 来选择的相应 HttpMessageConverter 来进 行的。 

### Http 和RPC框架的区别

虽然现在服务间的调用越来越多的使用了RPC 和消息队列, 但是HTTP 仍然有适合它的场景. 

RCP 的优势在于高效的网络传输模型(常使用NIO来实现),以及针对服务调用场景专门设计协议和高效的序列化技术. 

HTTP的优势在于它的成熟稳定、使用简单、被广泛支持、兼容性好、防火墙友好、消息的可读性高. 所以HTTP 协议在开放API、跨平台的服务调用,对性能要求不苛刻的场景中有广泛的使用.

## 微服务通信带来的问题

有了远程通信以后, 我们势必会考虑几个问题:

1. 目标服务肯定会做扩容,扩容以后,客户端会带来一些变化.
2. 客户端对于目标服务如何进行负载均衡.
3. 客户端如何维护目标服务的地址信息
4. 服务端的服务状态变化, 如何让客户端尽心感知.

![](http://files.luyanan.com//img/20191104103842.png)

## 引入注册中心

服务注册中心主要用于实现服务的注册和服务的发现功能, 在微服务架构中, 它起到了非常大的作用. 

### 注册中心的实现

Dubbo 体系中的Zookeeper、SpringCloud 中的Eureka 和Consul

##  重新认识Zookeeper

### Zookeeper 的前生今世

Apache Zookeeper 是一个高可用的分布式协调中间件。 它是Google Chubby 的一个开源实现, 那么它主要是解决什么问题的呢? 那就得先了解 Google Chubby

Google Chubby 是谷歌的一个用来解决分布式一致性问题的组件, 同时, 也是粗粒度的分布式锁服务. 

### 分布式一致性问题

什么是分布式一致性问题呢? 简单来说, 就是在一个分布式系统中, 有多个节点,每个节点都会提出一个请求,但是在所有节点中只能确定一个请求被通过.而这个通过是需要所有节点达成一致的结果. 所以所谓的一致性就是在提出的所有的请求中能够选出最终一个请求, 并且这个请求选出来以后, 所有的节点都要知道. 

这个就是典型的拜占庭问题. 

拜占庭将军问题说的是：拜占庭帝国军队的将军们必须通过投票达成一致来决定是否对某一个国家发起进攻. 但是这些将军在地理位置上是分开的, 并且在将军中存在叛徒. 叛徒可以通过任意行动来达到自己的目标. 

1. 欺骗某些将军采取进攻行动. 
2. 促使一个不是所有将军都同意的决定, 比如将军们本意是不希望进攻, 但是叛徒可以促成进攻行动. 
3. 迷惑将军使得他们无法做出决定.

如果叛徒达到了任意一个目标, 那么这次行动必然失败. 只有完全达成一致那么这次进攻才可能胜利. 

拜占庭问题是本质是,由于网络通信存在不可靠的问题, 也就是可能存在消息丢失, 或者网络延迟. 如何在这样的背景下对某一个请求达成一致. 

为了解决这个问题, 很多人提出了各种协议, 比如大名鼎鼎的Paxos. 也就是在不可信的网络环境中, 按照paxos 这个协议能够针对某个提议达成一致. 

所以分布式一致性的本质就是在分布式系统中, 多个节点就某一个提议如何达成一致.

> 这个和Google Chubby  有什么关系呢？ 

在Google  有一个GFS(Google file system), 他们有一个需求就是要从多个 GFS server中选出一个master Server . 这个就是典型的一致性问题, 5个分布在不同节点的server,需要确定一个master server,而他们要达成的一致性目标是: 确定某一个节点为master,并且所有节点要同意. 

而GFS 就是使用chubby 来解决这个问题的. 

**实现原理**:

所有的server 通过chubby 提供的通信协议到Chubby Server 上创建同一个文件, 当然, 最终只有一个server 能够获取创建的这个文件. 这个server 就成为了master, 它会在这个文件中写入自己的地址, 这样其他的server 通过读取这个文件就能知道被选出master 的地址. 

![](http://files.luyanan.com//img/20191104112256.png)



### 分布式锁服务

从另外一个层面来看, Chubby 提供了一种粗粒度的分布式锁服务, Chubby 是通过创建文件的形式来提供锁的功能. server 向chubby 中创建文件其实就表示加锁操作， 创建文件成功表示抢占到了锁. 

由于Chubby 没有开源, 所以雅虎公司就基于Chubby 的思想, 开发出了一个类似的分布式协调组件Zookeeper, 后面捐赠给了Apache

所以,大家一定要了解, zookeeper 并不是作为注册中心而设计的, 而是作为分布式锁的一种设计. 而注册中心只是它能实现的一种功能而已. 

## Zookeeper 的设计猜想

基于Zookeeper 本身的一个设计目标, zookeeper主要是解决分布式环境下的服务协调问题而产生的, 我们来猜想一下, 如果我们要去设计一个zookeeper, 需要满足哪些功能呢?

###  防单点故障

首先, 在分布式架构中, 任何的节点都不能以单点的方式存在, 因此我们需要解决单点的问题. 常见的解决单点的问题的方式就是集群. 

大家来思考一下, 这个集群需要满足哪些功能? 

1. 集群中要有主节点和从节点(也就是集群要有角色)
2. 集群要能做到数据同步, 当主节点出现故障, 从节点能够顶替主节点继续工作, 但是继续工作的前提是数据必须要主从节点保持一致. 
3. 主节点挂了以后, 从节点如何接替成为主节点, 是人工干预还是自动选举? 

所以基于这几个问题, 我们先把zookeeper 的集群节点画出来. 

###  数据同步

接着上面的那个结论思考, 如果要满足这样的一个高性能集群, 我们最直观的想法应该是每个节点都能接收到请求, 并且每个节点的数据都必须要保持一致. 要实现各个节点的数据一致性, 就势必要一个leader 节点负责协调和数据同步操作. 这个我向大家知道,如果在这样一个集群中没有leader节点，每个节点都可以接受所有请求, 那么这个集群的数据同步的复杂度是非常大的. 

所以, 当客户端请求过来, 需要满足,事务性数据和非事务性数据的分开处理方式, 就是leader 节点可以处理事务和非事务性数据.而follower 节点只能处理非事务性数据. 原因是: 对于数据变更的操作, 应该由一个节点来维护, 使得集群数据处理的简化. 同时,数据需要能能够通过leader 进行分发使得数据在集群中各个节点的一致性. 

![](http://files.luyanan.com//img/20191106095911.png)

leader节点如何与其他节点保证数据一致性,并且要求是强一致的. 在分布式系统中, 每一个机器节点虽然都能够明确知道自己进行的事务操作是成功还是失败,但是却无法直接获取其他分布式节点的操作过程. 所以当一个事务操作涉及到跨节点的时候, 就需要用到分布式事务, 分布式事务的数据一致性协议有2PC 协议和3PC协议. 

####  关于2PC 提交

(Two Phase Commitment Protocol) 当一个事务操作需要跨域多个分布式节点的时候, 为了保证事务处理的ACID 特性, 就需要引入一个"协调者"(TM)来统一调度所有分布式节点的执行逻辑,这些被调度的分布式节点被称为AP。 TM 负责调度AP的行为, 并最终决定这些AP是否要把事务真正进行提交,因为整个事务是分为两个阶段提交, 所以叫2PC。

![](http://files.luyanan.com//img/20191106100832.png)

![](http://files.luyanan.com//img/20191106101036.png)



#####  阶段一: 提交事务请求(投票)

1. 事务询问 

   > 协调者向所有的参与者发送事务内容, 询问是否可以执行事务提交操作, 并开始等待各参与者的响应. 

2. 执行事务

   > 在这个阶段, 协调者会根据和参与者的反馈情况来决定最终是否可以进行事务提交操作, 正常情况下包含两种可能: 执行事务、中断事务. 

   

 ### 角色

#### Leader  角色

Leader 服务器是整个zookeeper 集群的核心, 主要的工作是有两项:

1. 事务请求的唯一调度和处理者, 保证集群事务处理的顺序性. 
2. 集群内部各服务器的调度者. 

#### Follower  角色

Follower  角色的主要职责是: 

1. 处理客户端非事务请求, 转发事务请求给leader 服务器
2. 参与事务请求proposal 的投票(需要半数以上服务器通过才能通知leader  commit 数据, Leader 发起的提案, 要求Follower 投票)
3. 参数Leader 选举的投票

#### Observer 角色

Observer 是Zookeeper3.3 开始引入的一个全新的服务器角色, 从字面来理解, 该角色充当了观察者的角色。 

观察Observel 集群中的最新状态变化并将这些状态变化同步到Observer 服务器上, Observer 的工作原理与follower 角色基本一致, 而它和follower 角色唯一的不同在于Observer 不参与任何形式的投票,包括事务请求Proposal 的投票和leader 选举的投票. 简单来说, observer 服务器只提供非事务请求服务, 通常在于不影响集群处理能力的前提下提升集群非事务处理能力. 

### Leader 选举

当Leader 挂了, 需要从其他follower 节点中选择一个新的节点进行处理, 这个时候就需要设计到leader 选举. 



### 集群组成

通常zookeeper 是由2n+1台server组成, 每个server 都知道彼此的存在。每个server 都维护的内存状态镜像以及持久化存储的事务日志和快照. 对于2n-1台server , 只要有 n+1 台(大多数) server 可用, 整个系统保持可用.  我们已经了解到, 一个zookeeper 集群如果要对外提供可用的服务, 那么集群中必须要有过半的集群正常工作并且彼此之间能够正常通信, 基于这个特性, 如果想搭建一个能够允许F台集群down 掉的集群, 那么就要部署2*F+1 台服务器构成的zookeeper 集群. 因此3台集群组成的zookeeper 集群能够在挂掉一台机器后仍然正常工作. 一个5台机器集群的服务,能够对2台机器坏掉的情况下进行容灾. 如果一台由6台服务构成的集群, 同样只能挂掉两台. 因此, 5台和6台在容灾能力上并没有优势, 反而增加了网络通信负担. 系统启动时, 集群中的server 会选举出一台server 为Leader, 其他的就作为follower(这里先不考虑observer 角色).

> 之所以要满足这样一个等式, 是因为一个节点要称为集群中的leader, 需要有超过集群过半的节点支持, 这个涉及到leader 的选举算法, 同时也涉及到事务请求的提交投票. 

