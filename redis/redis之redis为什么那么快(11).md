# redis为什么那么快

## 1. redis到底有多快? 

https://redis.io/topics/benchmarks

```bash
cd /usr/local/soft/redis-5.0.5/src
redis-benchmark -t set,lpush -n 100000 -q
```



结果(本地虚拟机)

```text
SET: 51813.47 requests per second —— 每秒钟处理 5 万多次 set 请求
LPUSH: 51706.31 requests per second —— 每秒钟处理 5 万多次 lpush 请求
```





```bash
redis-benchmark -n 100000 -q script load "redis.call('set','foo','bar')"
```

结果（本地虚拟机）：

```text
script load redis.call('set','foo','bar'): 46816.48 requests per second —— 每秒钟 46000 次 lua 脚本调用
```

![image-20200330155417062](http://files.luyanan.com//img/20200330155438.png)

根据官方的数据, redis的QPS 可以达到10万左右(每秒请求数). 



##  2.redis为什么这么快?

总结来说有三点: 

- 纯内存结构
- 单线程
- 多路复用



### 2.1 纯内存结构

KV结构的内存数据库, 时间复杂度为`O(1)`.

第二个, 要实现这么高的并发性能, 是不是要创建非常多的线程? 恰恰相反, redis是单线程的.



### 2.2 单线程

单线程有什么好处呢? 

1. 没有创建线程、销毁线程带来的消耗. 
2. 避免了上下文切换带来的cpu消耗
3. 避免了线程之间带来的竞争问题, 例如加锁, 释放锁, 死锁等等. 

### 2.3 异步非阻塞I/O, 多路复用处理并发连接. 



## 3. redis为什么是单线程的?

不是白白浪费了CPU的资源呢? 

https://redis.io/topics/faq#redis-is-single-threaded-how-can-i-exploit-multiple-cpu--cores 

因为单线程已经够用了, CPU不是redis的瓶颈. redis 的瓶颈最有可能是机器内存或者网络带宽. 既然单线程容易实现, 而且CPU 不会成为瓶颈, 那就顺理成章的采用单线程的方案了. 



## 4.  单线程为什么这么快? 

因为redis 是基于内存的操作, 我们先从内存开始说起. 

### 4.1 虚拟存储器(虚拟内存`Vitual Memory`)

名词解释： 主存: 内存; 辅存:磁盘(硬盘)

计算机内存(主存) 可看作一个由M个连续的字节大小的单元组成的数组, 每个字节都有一个唯一的地址, 这个地址叫做物理地址(`PA`). 早期的计算机中, 如果CPU 需要内存, 使用物理寻址, 直接访问主存储器.

![image-20200330170002604](http://files.luyanan.com//img/20200330170004.png)

这种方式有几个弊端: 

1. 在多用户多任务的操作系统中, 所有的进程共享内存, 如果每个进程都独占一块物理地址空间, 主存很快就会被用完. 我们希望在不同的时刻, 不同的进程都可以共用同一个物理地址空间. 
2. 如果所有的进程都是直接访问物理地址, 那么一个进程就可以修改其他物理内存的内存数据, 导致物理地址空间被破坏, 程序运行就会出现异常. 

为了解决这个问题, 我们就想到了一个办法, 在CPU和主存之间增加一个中间层. CPU不再使用物理内存, 而是访问一个虚拟地址, 由这个中间层将地址转换为物理地址, 最终获得数据. 这个中间层就叫做虚拟存储器(`Virtual Memory`)

具体的操作如下所示： 

![image-20200330171937847](http://files.luyanan.com//img/20200330171939.png)

在每一个进程开始创建的时候, 都会分配一段虚拟地址, 然后通过虚拟地址和物理地址的映射来获取真实的数据, 这样进程就不会直接接触到物理地址, 甚至不知道自己调用的哪块物理地址的数据, 

目前, 大多数操作系统都是用了虚拟内存, 如`window`系统的虚拟内存、linux 系统的交换空间等等. windows的虚拟内存(`pagefile.sys`) 是磁盘空间的一部分. 

在32位的系统上, 虚拟地址空间大小是`2^32bit=4G`. 在64位的系统上, 最大虚拟地址空间大小是多少? 是不是`2^64bit=1024*1014TB=1024PB=16EB`? 实际上并没有用到64位, 因为用不到这么大的空间，而且会造成很大的系统开销。 Linux 一般用低48位来表示虚拟地址, 也就是`2^48bit=256T`. 

```bash
cat /proc/cpuinfo
```

`address sizes : 40 bits physical, 48 bits virtual`

实际的物理内存可能远远小于虚拟 内存的大小. 

总结: 引入虚拟内存, 可以提供更大的地址空间, 并且地址空间是连续的, 使得程序编写、连接更加简单. 并且可以对物理内存进行隔离, 不同的进程操作互不影响. 还可以通过把同一快物理内存映射到不同的虚拟地址空间实现内存共享. 



###  4.2 用户空间和内核空间

为了避免用户进程直接操作内核, 保证内核安全, 操作系统将虚拟内存划分为两个部分, 一部分是内核空间(`间（Kernel-space）/ˈkɜːnl /`),一部分是用户空间(`User-space`). 

![image-20200330174610659](http://files.luyanan.com//img/20200330174612.png)

内核是操作系统的核心, 独立于普通的应用程序, 可以访问受保护的内存空间, 也有访问底层硬件设备的权限. 

内核空间中存放的是内核代码和数据, 而进程的用户控件中存放的是用户程序的代码和数据. 不管是内核空间还是用户空间, 他们都处于虚拟空间中, 都是对物理的映射 . 

在linux系统中, 内核进程和用户进程所占的虚拟内存比例是 1:3

![image-20200330175838075](http://files.luyanan.com//img/20200330175839.png)

当进程运行在内核空间的时候就处于内核态, 而进程运行在用户空间则处于用户态. 

进程在内核空间可以执行任意命令, 调用系统的一切资源: 在用户空间只能执行简单的运算, 不能直接调用系统资源, 必须通过调用系统接口(又称`system call`), 才能向内核空间发出指令. 

top命令: 

![image-20200330180355553](http://files.luyanan.com//img/20200330180356.png)



- us: 代表CPU 消耗在`User space` 的时间百分比
- `sy`: 代表CPU消耗在`Kernel space`的时间百分比. 



### 4.3 进程切换(上下文切换)

多任务操作系统是怎么实现运行远大于CPU数量的任务个数的? 当然， 这样任务实际上并不是真的在同时运行, 而是因为系统通过时间片分片算法, 在很短的时间内, 将CPU轮流分配给他们, 造成多任务同时运行的假象. 

![image-20200330180724897](http://files.luyanan.com//img/20200330180726.png)

为了控制进程的执行, 内核必须有能力挂起正在CPU上运行的进程, 并恢复以前挂起的某个进程的执行, 这种行为被称为进程切换. 

什么叫上下文? 

在每个任务运行前, CPU都需要知道任务从哪里加载, 又从哪里开始运行, 也就说, 需要系统事先帮他设置好CPU 寄存器和程序计数器(`Program Counter`),这个叫做CPU的上下文. 	

而这些保存下来的上下文, 会存储在系统内核中, 并在任务重新调度执行时再次加入加载进来. 这样就能保证任务原来的状态不受影响, 让任务看起来还是连续运行的. 

在切换上下文到时候, 需要完成一系列的动作, 这是一个很消耗资源的操作. 



###  4.4 进程的阻塞

正在运行的进程由于提出系统服务i请求(如I/O操作),但因为某种原因未得到操作系统的立即响应, 该进程只能将自己编程阻塞状态, 等待响应的事件出现后才被唤醒, 进程在阻塞状态不占CPU.



### 4.5  文件描述符(`FD`)

linux 系统将所有设备都当作文件处理, 而linux 用文件描述符来标识每个文件对象. 

文件描述符(`File Descriptor`) 是内核为了高效管理已经被打开的文件所创建的索引, 用于指向被打开的文件, 所有执行i/O 操作的系统调用都是通过文件描述符, 文件描述符是一个简单的非负整数, 用来表明每个被进程打开的文件

linux系统里面有三个标准的文件描述符

- 0: 表示输入(键盘)
- 1: 标准输出(显示器)
- 2: 标准错误输出(显示器)



### 4.6  传统I/O 数据拷贝

以读操作为例： 

当应用程序执行read系统调用读取文件描述符(`FD`) 的时候, 如果这块数据已经存在于用户进程的页内存中, 就直接从内存中读取数据. 如果数据不存在, 则先将数据从磁盘加载数据到内核缓冲区中, 再从内核缓存区拷贝到用户进程的页内存中(两次拷贝,两次user和`kernel`的上下文切换)

![image-20200330192750824](http://files.luyanan.com//img/20200330192754.png)

I/O 的阻塞到底阻塞在哪里呢? 

### 4.7 Blocking I/O

当使用`read` 或者`write`对某个文件描述符进行读写时, 如果当前`FD` 不可读,系统就不会对其他的操作做出响应. 从设备复制数据到内核缓冲区是阻塞,从内核缓存区拷贝到用户空间也是阻塞的. 直到`copy complete`,内核返回结果, 用户进程才解除`block`的状态. 

![image-20200330194128642](http://files.luyanan.com//img/20200330194130.png) 

为了解决阻塞的问题, 我们有几个思路： 

1. 在服务端创建多个线程或者使用线程池, 但是在高并发的情况下需要的线程会很多, 系统无法承受, 而且创建和释放线程都需要消耗资源. 
2. 由请求方定期轮询, 在数据准备完毕后, 再从内核缓冲区复制数据到用户空间(非阻塞式I/O), 这种方式会存在一定的延迟. 

能不能用一个线程处理多个客户端请求呢? 

### 4.8  I/O多路复用(`I/O Multiplexing`)

I/O 指的是网络I/O

**多路:** 指的是多个TCP连接(Socket 或者Channel)

**复用:** 指的是复用一个或者多个线程

他的基本原理就是不再由应用程序自己监视连接, 而是由内核替应用程序监视文件描述符. 

客户端在操作的时候, 会产生具有不同事件的socket. 在服务端, I/O 多路复用程序(`I/O Multiplexing Module`)  会把消息放入到队列中, 然后通过文件事件分派器(`File event Dispatcher`), 转发到不同的事件处理器中. 

![image-20200330200152471](http://files.luyanan.com//img/20200330200153.png)



多路复用有很多的实现, 以`select`为例, 当用户进程调用了多路复用器, 进程会被阻塞. 内核会监视多路复用器负责的所有socket, 当任何一个socket的数据准备好了, 多路复用器就会返回. 这个时候用户再调用`read`操作, 把数据从内核缓冲区拷贝到了用户空间. 

![image-20200330201320838](http://files.luyanan.com//img/20200330201322.png) 

所以, I/O多路复用的特点就是通过一种机制一个进程能够同时等待多个文件描述符,而这些文件描述符(套接字描述符)其中的任意一个进入读就绪(`readable`) 状态, `select()`  函数就可以返回. 

redis 的多路复用,提供了`select`.`epoll`,`evport`,`kqueue`几种选择, 在编译的时候来选择一种, 源码`ae.c`

```c
/* Include the best multiplexing layer supported by this system.
 * The following should be ordered by performances, descending. */
#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
```

- evport 是 Solaris 系统内核提供支持的； 

- epoll 是 LINUX 系统内核提供支持的； 

- kqueue 是 Mac 系统提供支持的； 

- select 是 POSIX 提供的，一般的操作系统都有支撑（保底方案）； 

源码 ae_epoll.c、ae_select.c、ae_kqueue.c、ae_evport.c