# redis 之分布式篇

## 1. 为什么需要redis集群



###  1.1  为什么需要集群呢? 

#### 1.1.1 性能

redis本身的`QPS` 已经很高了, 但是如果在一些并发量非常高的情况下, 性能还是会收到影响的, 这个时候我们希望有更多的redis服务来完成工作. 



####  1.1.2  扩展

第二个是处于存储的考虑, 因为redis 所有的数据都是存储到内存中,如果数据量大 , 很容易收到硬件的限制, 升级硬件成效和成本比太低, 所有我们需要有一种横向扩展的方法. 



####  1.1.3 可用性

第三个是可用性和安全 的问题, 如果只有一个redis节点, 一旦服务宕机, 那么所有的客户端都无法访问, 会对业务造成很大的影响. 另一个, 如果硬件发生故障, 而单机的数据也是无法恢复, 带来的影响也是灾难性的. 

可用性,数据安全,性能都可以通过搭建多个redis服务来实现, 其中一个是主节点(master), 可以有多个从节点(slave),  主从节点通过数据复制, 存储完全相同的数据, 如果节点发生故障, 则将某个从节点改成主节点, 访问新的主节点. 



## 2. redis主从复制(`replication`)

### 2.1 主从复制配置

例如一主多从, 203是主节点, 在每个`slave` 节点上的`redis.conf` 配置文件中增加一行. 

```properties
slaveof 192.168.8.203 6379

```

在主从切换的时候, 这个配置就会被重写为: 

```properties
# Generated by CONFIG REWRITE
replicaof 192.168.8.203 6379
```

 或者在启动服务的时候通过参数指定master节点

```properties
./redis-server --slaveof 192.168.8.203 6379
```

或者在客户端直接执行`slaveof xx xx`, 使得redis实例成为从节点.

启动后, 查看集群的状态. 

```bash
redis> info replication

```

 从节点不能写入数据(只读), 只能从master节点同步数据.`get` 成功, `set` 失败 . 

```bash
127.0.0.1:6379> set test 666
(error) READONLY You can't write against a read only replica.
```

主节点写入后，`slave` 会自动从`master` 同步数据. 

断开复制

```bash
redis> slaveof no one

```

此时从节点会变成自己的主节点, 不再复制数据. 



### 2.2 主从复制原理

####  2.2.1 连接阶段

1. `slave node`  启动时(执行`slaveof` 命令), 会在自己本地保存`master node` 的信息, 包括`master node` 的`host` 和`ip`. 
2. `slave node` 内部有个定时任务`replicationCron` （源码`replication.c`）, 每隔一秒钟检查是否有新的`master node` 要连接和复制, 如果发现, 就跟`master node`建立`socket` 网络连接, 如果连接成功, 从节点为该`socket` 建立一个专门处理复制工作的文件时间处理器,负责后续的复制工作, 如接受`RDB` 文件、接受命令传播等. 

当从节点变成了主节点的一个客户端后, 会给主节点发送`ping` 请求. 



####  2.2.2 数据同步阶段

3. `master node` 第一次执行全量复制,通过`bgsave` 命令在本地生成一份`PDB` 快照, 将`RDB` 快照发送给`slave node`(如果超时会重连, 可以调大`repl-timeout`的值). `slave node` 首先清除自己的旧数据, 然后用`RDB` 文件加载数据. 

**问题: 生成`PDB`期间, `master` 接收到的命令怎么处理? **

开始生成`RDB` 文件时, `master`会把所有新的写命令缓存在内存中, 在`slave node`保存了`RDB` 之后, 再将新的写命令复制给 `slave node`, 



#### 2.2.3 命令传播阶段

4. `master node`持续将写命令, 异步复制给`slave node`

   延迟是不可避免的, 只能通过优化网络. 

```properties
repl-disable-tcp-nodelay no
```

当设置为`yes`的时候, `TCP`会对包进行合并从而减少带宽, 但是发送的频率会降低, 从节点数据延迟增加，一致性变差. 具体发送频率与`linux`内核的配置有关, 默认配置为40ms.当设置为`no`时, TCP 会立马将主节点的数据发送给从节点, 带宽增加但是延迟变小. 

一般来说, 只有当应用对redis数据不一致的容忍度较高,且主从节点之间的网络状态不好时, 才会设置为yes, 大多数情况下为no

**问题: 如果从节点有一段时间断开了与主节点的连接是不是要重新全量复制一遍, 如果可以增量复制, 怎么知道上次复制到了哪里？**

通过`master_repl_offset`  记录的偏移量

```bash
redis> info replication

```

![image-20200401120152712](http://files.luyanan.com//img/20200401120153.png)



### 2.3  主从复制的不足

主从模式解决了数据备份和性能(通过读写分离)的问题, 但是还是存在一些不足. 

1. `RDB` 文件过大的情况下, 同步非常耗时. 
2. 在一主一从或者一主多从的情况下， 如果主服务器挂了，对外提供的服务就不可用了. 单点问题没有得到解决. 如果每次都是手动把之前的从服务器切换为主服务器, 这个比较费时费力, 还会造成一定时间的服务不可用. 



## 3. 可用性保证值sentinel

### 3.1 Sentinel 原理

如何实现主从的自动切换呢? 我们的思路：

创建一台监控服务器来监控所有的Redis服务器的节点状态, 比如`master`节点超过一定时间没有给监控服务区发送心跳报文, 就把`master` 标记为下线, 然后把某一个`slave` 变成`master`.应用每一次都是从这个监控服务器拿到`master`的地址. 

问题是: 如果监控服务器本身出了问题怎么办? 那我们就拿不到`master`的地址了,应用也就没办法访问了. 

那我们就在创建一个监控服务器, 来监控监控服务器...似乎陷入了死循环中, 这个问题怎么解决? 这个问题先放着. 

redis 的`sentinel` 就是这种思路, 通过运行监控服务器来保证服务的可用性. 

[官网-sentinel的介绍](https://redis.io/topics/sentinel)

从redis2.8版本起, 提供了一个稳定版本的`sentinel`(哨兵),用来解决高可用的问题, 它是一个特殊状态的redis实例. 

我们会启动一个或者多个`sentinel`的服务(通过`src/redis-sentinel`),它本质上只是一个运行在特殊模式下的redis, `sentinel` 通过`info`命令得到被监听的redis机器的`master` 和`slave`等信息. 

![image-20200401125108889](http://files.luyanan.com//img/20200401125110.png)

为了保证监控服务的可用性, 我们会对`sentinel` 做集群的部署. `sentinel` 既可以监控所有的redis服务, 也可以互相监控. 

注意:`sentinel` 本身没有主从之分,只有redis 节点才会有主从之分. 



#### 3.1.1 服务下线

`sentinel` 默认以每秒钟1次的频率向redis服务节点发送`ping` 命令。如果在`down-after-milliseconds` 内都没有收到有效回复, `sentinel` 会将该服务标记为下线(**主观下线**)

```properties
# sentinel.conf
sentinel down-after-milliseconds <master-name> <milliseconds>
```

这个时候`sentinel`节点会继续询问其他的`sentinel` 节点, 确认这个节点是否下线, 如果多数的`sentinel`节点都认为`master` 下线, `master` 才真正的确认为下线(**客观下线**), 这个时候就需要重新选举`master`. 



#### 3.1.2 故障转移

如果`master` 标记为下线,就会开始故障转移流程. 

既然有那么多的`sentinel`节点, 那么由谁进行故障转移的事情呢? 

故障转移的第一步就是在`sentinel` 集群里面选择出一个`leader`,  由`leader` 完成故障转移流程. `sentinel` 通过`Raft`算法, 实现`sentinel` 选举. 



##### `Raft` 算法

在分布式存储系统中, 通常需要维护多个副本来提高系统的可用性, 那么多个节点之间必须要面对数据一致性的问题. `Raft`的目的就是通过复制的方式,使得所有的节点达成一致, 但是那么多的节点，以哪个节点的数据为准呢? 所以必须选出一个`leader`. 

大体上有两个步骤: 领导选举, 数据复制. 

`Raft` 是一个共识算法(`consensus algorithm`), 例如比特币之类的加密货币, 就需要共识算法. `Spring Cloud` 的注册中心解决方案`Consul` 也用到了`Raft` 算法. 

`Raft` 的核心思想就是: 先到先得, 少数服从多数. 

[`Rafe`算法演示](http://thesecretlivesofdata.com/raft/)

总结： 

`sentinel`的`Raft` 算法和`Raft` 论文略有不同. 

1. `master` 客观下线触发选举, 而不是过了`election timeout` 时间开始选举
2. `leader` 并不会把自己成为`leader`的消息发送给其他`sentinel`,  其他`sentinel` 等待`leader` 从`slave`选出`master`后, 检测到新的`master`正常工作后, 就会去掉客观下线的标识, 从而不需要进入故障转移流程. 

##### 故障转移

**问题:怎么让一个原来的`slave` 节点成为主节点呢? **

1. 选出`sentinel leader` 之后, 由`sentinel leader` 向某个节点发送`slaveof no one` 命令, 让他成为独立的节点. 
2. 然后向其他节点发送`slaveof x.x.x.x xxxx `（本机服务）, 让他们成为这个节点的子节点, 故障转移完成. 

**问题： 那么多从节点, 选谁成为主节点? **

关于从节点选举, 一共有**四个因素影响选举的结果**, 分别是断开连接时长, 优先级排序、复制数量、进程id

如果与哨兵连接断开的时间比较久, 超过了某个阈值, 就直接失去了选举权. 如果拥有选举权, 那就看谁的优先级高, 这个配置文件里面配置设置(`replica-priority 100`), 数值越小优先级越高. 

如果优先级相同, 就看谁从`master` 中复制的数据最多(复制偏移量最大), 选最多的那个, 如果复制的数量相同,那就选择进程id最小的那个. 



###  3.2 `sentinel` 的功能总结

- Monitoring. Sentinel constantly checks if your master and slave instances are working as expected
- Notification. Sentinel can notify the system administrator, another computer programs, via an API, that something is wrong with one of the monitored Redis instances.
- Automatic failover. If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting.
- Configuration provider. Sentinel acts as a source of authority for clients service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address.



监控: `sentinel` 会不断检查主服务器和从服务器是否正常运行, 

通知： 如果某一个被监视的实例出现了问题, `sentinel` 可以通过`API`发出通知. 

自动故障转移(`failover` ): 如果主服务器发生故障, `sentinel` 可以启动故障转移过程, 把某台服务器升级成主服务器  并发出通知. 

配置管理：客户端连接到`sentinel` , 获取当前的redis 主服务器的地址. 



### 3.3 `sentinel` 实战

#### 3.3.1 `sentinel` 配置

为了保证`sentinel` 的高可用, `sentinel`也需要做集群部署, 集群中至少需要三个`sentinel`实例(推荐奇数个, 防止脑裂)。

| hostname | ip地址        | 节点角色和端口                    |
| -------- | ------------- | --------------------------------- |
| master   | 192.168.8.203 | Master：6379 / Sentinel : 2637    |
| slave1   | 192.168.8.204 | `Slave ：6379 / Sentinel : 26379` |
| slave2   | 192.168.8.205 | Slave ：6379 / Sentinel : 26379   |



以redis安装路径 `/usr/local/soft/redis-5.0.5/` 为例 

在204、205的`src/redis.conf` 配置文件中添加

```properties
slaveof 192.168.8.203 6379
```

在203、204、205 创建`sentinel`配置文件(安装后根目录下默认有`sentinel,conf`):

```bash
cd /usr/local/soft/redis-5.0.5
mkdir logs
mkdir rdbs
mkdir sentinel-tmp
vim sentinel.conf
```

三台服务器的内容相同

```properties
daemonize yes
port 26379
protected-mode no
dir "/usr/local/soft/redis-5.0.5/sentinel-tmp"
sentinel monitor redis-master 192.168.8.203 6379 2
sentinel down-after-milliseconds redis-master 30000
sentinel failover-timeout redis-master 180000
sentinel parallel-syncs redis-master 1
```

上面出现了4个`redis-master`,这个名称要统一, 并且使用客户端(比如jedis),连接的时候名称要正确. 

- `protected-mode`

  > 是否允许外部网络访问

- `dir`

  > `sentinel` 的工作目录. 

- `sentinel monitor`

  > `sentinel` 监控的redis 主节点

- `down-after-milliseconds（毫秒）`

  > 1. 同一个`sentinel` 对同一个`master` 两次`failover` 之间的间隔时间
  > 2. 当一个`slave` 从一个错误的`master` 那里同步数据开始计算时间,直到`slave` 被纠正为向正确的`master`那里同步数据. 
  > 3. 当想要取消一个正在进行的`failover` 所需要的时间
  > 4. 当进行`failover`时,配置所有的`slaves` 指向新的`master` 所需的最大时间. 

- `parallel-syncs`

  > 这个配置指定了在发生`failover` 主备切换时最多可以有多少个`slave` 同时对新的`master` 进行同步, 这个数字越小, 完成`failover`所需的时间就越长,但是如果这个数字越大, 就意味着越多的`slave` 因为`replication` 而不可用. 可以通过将这个值设置为1来保证每次只有一个`salve`处于不能处理命令请求的状态. 



#### 3.3.2 sentinel验证

启动redis服务和sentinel服务

```bash
cd /usr/local/soft/redis-5.0.5/src
# 启动 Redis 节点
./redis-server ../redis.conf
# 启动 Sentinel 节点
./redis-sentinel ../sentinel.conf
# 或者
./redis-server ../sentinel.conf --sentinel
```

查看集群状态

```bash
redis> info replication
```

**203**

![image-20200401143026937](http://files.luyanan.com//img/20200401143028.png)

**204和205**

![image-20200401143054178](http://files.luyanan.com//img/20200401143055.png)

模拟`master` 宕机, 在203执行

```bash
redis> shutdown

```



205 被选出新出`master`, 只有一个`slave`节点

![image-20200401143221387](http://files.luyanan.com//img/20200401143222.png)

注意看，`sentinel.conf` 里面的`redis-master`被修改了. 

模拟原`master` 恢复, 在203启动`redis-server` , 它还是`slave`,但是`master`又有两个`slave`了. 



#### 2.3.3 sentinel 连接使用

`jedis` 连接`sentinel`

`master name`来自于`sentinel.conf` 的配置

```java
/**
 * @author luyanan
 * @since 2020/4/1
 * <p>使用jedis连接sentinel</p>
 **/
public class JedisSentinelTest {

    private static JedisSentinelPool pool;

    private static JedisSentinelPool createPool() {
        String masterName = "redis-master";
        Set<String> sentinels = new HashSet<>();
        sentinels.add("192.168.8.203:26379");
        sentinels.add("192.168.8.204:26379");
        sentinels.add("192.168.8.205:26379");
        pool = new JedisSentinelPool(masterName, sentinels);
        return pool;
    }


    public static void main(String[] args) {
        JedisSentinelPool pool = createPool();
        pool.getResource().set("test", "" + System.currentTimeMillis());
        System.out.println(pool.getResource().get("test"));
    }

}
```



`SpringBoot` 连接`sentinel` 

```properties
spring.redis.sentinel.master=redis-master
spring.redis.sentinel.nodes=192.168.8.203:26379,192.168.8.204:26379,192.168.8.205:26379
```

无论是`jedis` 还是`Springboot` (2.X版本默认是`lettuce`),都只需要配置全部哨兵的地址，由哨兵返回当前的`master节点地址. 



### 3.4  哨兵机制的不足

主从切换的的过程中丢失数据, 因为只有一个`master`

只能单点写, 没有解决水平扩容的问题. 

如果数据量比较大, 这个时候我们需要多个`master-slave`的`group`,  把数据分不到不同的`group`

问题来了, 数据怎么分片? 分片之后, 怎么实现路由呢 ? 



## 4. redis 分布式方案

如果要实现redis 数据的分片, 我们有三种方案. 第一种是在客户端实现相关的逻辑, 例如用取模或者一致性哈希对key进行分片, 查询和修改都先判断key 的路由. 

第二种是把分片处理的逻辑抽取出来,运行一个独立的代理服务, 客户端连接到这个代理服务, 代理服务做请求的转发. 

第三种就是基于服务端实现. 

### 4.1客户端`Sharding`

![image-20200401151402444](http://files.luyanan.com//img/20200401151404.png)  

`jedis` 客户端提供了`redis sharding`的方案, 并且支持连接池. 

```java
/**
 * @author luyanan
 * @since 2020/4/1
 * <p>分片处理</p>
 **/
public class ShardingTest {

    public static void main(String[] args) {
        JedisPoolConfig poolConfig = new JedisPoolConfig();
        //redis服务器
        JedisShardInfo shardInfo1 = new JedisShardInfo("127.0.0.1", 6379);
        JedisShardInfo shardInfo2 = new JedisShardInfo("192.168.8.205", 6379);
        // 连接池
        List<JedisShardInfo> infos = new ArrayList<>();
        infos.add(shardInfo1);
        infos.add(shardInfo2);

        ShardedJedisPool shardedJedisPool = new ShardedJedisPool(poolConfig, infos);
        ShardedJedis jedis = null;
        try {
            jedis = shardedJedisPool.getResource();
            for (int i = 0; i < 100; i++) {
                jedis.set("key-" + i, "" + i);
            }

            for (int i = 0; i < 100; i++) {
                Client client = jedis.getShard("key-" + i).getClient();
                System.out.println("取到值:" + jedis.get("key-" + i) + ",当前key处于:" + client.getHost() + ":" + client.getPort());
            }
        } finally {

            if (jedis != null) {
                jedis.close();
            }
        }

    }

}

```



使用`shardedJedis` 之类的客户端分片代码的优势是配置简单, 不依赖其他中间件, 分片的逻辑可以自己定义, 比较灵活.但是基于客户端的方案, 不能实现动态的服务递减,每个客户端需要自行维护分片策略, 存在重复代码. 

第二种思路就是把分片的代码抽取出来, 做成一个公共的服务,所有的客户端都连接到这个代码层, 由代理层来实现请求和转发. 



### 4.2 代理Proxy

![image-20200401153429660](http://files.luyanan.com//img/20200401153431.png)



典型的代理分区方案有`Twitter`开源的`Twemproxy` 和国内的豌豆荚开源的`Codis`



#### 4.2.1 `Twemproxy`

[`two-em-proxy` git地址](https://github.com/twitter/twemproxy)

![image-20200401155058609](http://files.luyanan.com//img/20200401155100.png)

Twemproxy的优点: 比较稳定, 可用性高. 

不足：

1. 出现故障不能自动转移,架构复杂，需要借助其他组件(`LVS/HAProxy + Keepalived`) 实现HA
2. 扩缩容需要修改配置, 不能实现平滑的扩缩容(需要重新分布数据). 



####  4.2.2 Codis

[`Codis` 官网地址](https://github.com/CodisLabs/codis)

`Codis`是一个代理中间层,用`GO`语言开发

功能: 客户端连接`Codis` 跟连接redis 没有区别. 

分片原理 `Codis` 把所有的key 分成了N个槽(例如1024), 每个槽对应一个分组，一个分组对应一个或者一组redis实例. `Codis` 对key 进行`CBC32` 运算，得到一个32位的数字, 然后摸以N(槽的个数),得到余数, 这个就是key对应的槽, 槽后面就是redis 的实例, 比如4个槽：

![image-20200401161323845](http://files.luyanan.com//img/20200401161324.png)

`Codis` 的槽位映射关系是保存在`proxy`中的, 如果要解决单点的问题，`Codis` 也要做集群部署, 多个`Codis` 节点怎么同步槽和实例的关系呢? 需要运行一个`zookeeper`（或者`etcd`/本地文件）. 

在新增节点的时候, 可以为节点指定特定的槽位. `Codis` 也提供了自动均衡策略. 

`Codis` 不支持事务, 其他的一些命令也不支持. 

不支持的命令.  https://github.com/CodisLabs/codis/blob/release3.2/doc/unsupported_cmds.md

获取数据原理(`mget`): 在redis 中的各个实例里获取到符合的key, 然后再汇总到`Codis`. 

`Codis`是第三方提供了分布式解决方案, 在官方的集群功能稳定之前, `Codis` 也得到了大量的应用. 



### 4.3 Redis Cluster

https://redis.io/topics/cluster-tutorial/

`redis cluster` 是在redis 3.0 的版本正式推出的, 用来解决分布式的需求, 同时也可以实现高可用. 跟`Codis`不一样, 它是去中心化的, 客户端可以连接到任意一个可用的节点. 

数据分片有几个关键的问题需要解决： 

1. 数据怎么相对均匀的分片
2. 客户端怎么访问到相应的节点和数据
3. 重新分片的过程,怎么保证正常服务. 



#### 4.3.1 架构

`Redis Cluster` 可以看成是由多个redis实例组成的数据集合, 客户端不需要关注数据的子集到底存储在哪个节点,只需要关注这个集合整体. 

以3主3从为例，节点之间两两交互,共享数据分片, 节点状态等信息. 

![image-20200401164829167](C:/Users/luyanan/AppData/Roaming/Typora/typora-user-images/image-20200401164829167.png)





####  4.3.2 集群命令

1. 集群命令

   - `cluster info`

     > 打印集群的信息

     `cluster nodes`  

     >  列出集群当前已知的所有节点(node), 以及这些节点的相关信息
   
2. 节点命令：

   - `cluster meet <ip> <port>`

     > 将ip和port 所指定的节点加入集群当中,让他成为集群的一份子。 
     
   - `cluster forget <node_id>`
   
     > 从集群中移除<node_id> 指定的节点的从节点. 
   
   -  `cluster replicate <node_id>`
   
     > 将当前节点设置为<node_id> 指定的节点的从节点
   
   - `cluster saveconfig`
   
     > 将节点的配置文件保存到硬盘中
   
3. 槽(slot）命令

   - `cluster addsolts <solt> [solt...]`

     >  将一个或者多个槽(slot) 指派(assign) 给当前节点

   - `cluster delsolts <solt>[solt...]`
   
     >  移除一个或者多个槽对当前节点的指派
   
   - `cluster flushsolts`
   
     >  移除指派给当前节点的所有槽, 让当前节点变成一个没有指派任何槽的节点
   
   - `cluster setsolt <solt> node <node_id>`
   
     >  将槽`solt` 指派给<node_id>指定的节点, 如果槽已经指派给了另一个节点, 那么先让另一个节点删除该槽, 然后在进行指派. 
   
   - `cluster setsolt <slot> migrating <node_id>`
   
     >  将本节点的槽`solt`迁移到<node_id> 指定的节点中
   
   - `cluster setsolt <slot> importing <node_id>`
   
     >  从<node_id> 指定的节点中导出槽`slot` 到本节点
   
   - `cluster setslot <slot> stable`
   
     > 取消对槽`slot` 的导入(`import`) 或者迁移`migrate`
   
4. 键命令

   - `cluster keysolt <key>`

     > 计算键key 应该被放置到哪个槽上

   - `cluster countkeysinslot <slot>`

     > 返回槽slot 目前包含的键值对数量

   - `cluster getkeysinslot <slot> <count>`

     >  返回count个slot槽中的键



####  4.3.3  数据分布

如果是希望数据分布相对均匀的话，我们首先可以考虑哈希后取模. 



#####  哈希后取模

例如`hash(key) %N`  , 根据余数, 决定映射到那一个节点, 这种方式比较简单, 属于静态的分片规则, 但是一旦节点数量发生变化, 新增或者减少, 由于取模的N 发生了变化, 数据就需要重新分布. 

为了解决这个问题, 我们就有了一致性哈希算法

##### 一致性哈希算法



一致性哈希的原理

把所有的哈希值空间组织称一个虚拟的圆环(哈希环). 整个空间按照顺时针方向组织. 因为是环形空间, 0和`2^32-1` 是重叠的. 

假设我们有四台机器要哈希环来实现映射(分布数据), 我们先根据机器的名称或者ip计算哈希值, 然后分布到哈希环中(红色圆圈)

![image-20200401172713531](http://files.luyanan.com//img/20200401172714.png)

现在有4条数据或者4个访问请求，对key计算后， 得到哈希环中的位置, 沿着哈希环顺时针找到第一个Node, 就是数据存储的节点. 

![image-20200401173437115](http://files.luyanan.com//img/20200401173438.png)

在这种情况下， 新增了一个Node5节点, 不影响数据的分布

![image-20200401173601739](http://files.luyanan.com//img/20200401173602.png)

删除了一个节点Node4.只影响到相邻的一个节点. 

![image-20200401173747850](http://files.luyanan.com//img/20200401173748.png)



谷歌的`MurmurHash` 就是一致性哈希算法, 在分布式系统中, 负载均衡、分库分表等场景中都有应用. 

一致性哈希算法解决了动态增减节点时候,所有的数据都需要重新分布的问题, 它只会影响到下一个相邻的节点, 对其他节点没有影响. 

但是这样的一致性哈希算法还有一个缺点, 因为节点不一定是均匀的分布的, 特别是在节点数量少的情况下,所以数据并不能得到均匀的分布. 解决这个问题的办法就是引入虚拟节点 (`Virtual Node`). 

比如2个节点，5条数据,只有1条分布到Node2, 4条分布到Node1, 不均匀. 

![image-20200401174535555](http://files.luyanan.com//img/20200401174538.png)



Node1 设置了两个虚拟节点, Node2 也设置了两个虚拟节点(虚拟圆圈)

这个时候有3条数据分布到了Node1,  1条数据分布到了Node2

![image-20200401175356125](http://files.luyanan.com//img/20200401175357.png)



**redis** 虚拟槽分区

redis 既没有使用哈希取模, 也没有用一致性哈希,而是使用虚拟槽来实现的。 

redis 创建了16384个槽(slot)， 每个节点负责一定区间的slot, 比如Node1负责0-5460, Node2 负责5461-10922, Node3 负责10923-16383. 

![image-20200401202743576](http://files.luyanan.com//img/20200401202745.png)

redis节点的每个`master`节点维护一个16384位(2048bytes=2KB)的位序列, 比如: 序列的第0位是1 , 就代表第一个`slot` 是他负责, 序列的第1位是0 ,代表第二个`slot` 不归他负责. 

对象分布到redis节点, 对key 用`CRC16` 算法计算再%16384,得到一个`slot` 的值 ， 数据落到负责这个的`slot`的redis节点上. 

查看key属于哪个`slot`

```bash
redis> cluster keyslot test	

```

注意: key与solt的关系是永远不会变得, 会变的只有`slot`与redis节点的关系. 

**问题：怎么让相同的数据落在同一个节点上? **

有些比如`multi key` 操作是不能跨节点的, 如果要让某些数据分布到一个节点上, 例如用户2673的基本信息和金融信息,怎么办? 

在key中加入`{has_tag}` 即可, redis在计算槽位编号的时候, 只会获取{}之间的字符串进行槽编号, 这样由于上面,两个不同的键, {} 里面的字符串是相同的, 因为他们可以计算出相同的槽. 

```bash
127.0.0.1:7293> set a{qs}a 1
OK
127.0.0.1:7293> set a{qs}b 1
OK
127.0.0.1:7293> set a{qs}c 1
OK
127.0.0.1:7293> set a{qs}d 1
OK
127.0.0.1:7293> set a{qs}e 1
OK

```

**问题: 客户端连接到哪一台服务器? 访问的数据不在当前节点上, 怎么办?**



#### 4.3.4  客户端重定向

比如在7291端的redis的`redis-cli`  客户端操作. 

```bash
127.0.0.1:7291> set qs 1
(error) MOVED 13724 127.0.0.1:7293
```

 服务端返回`MOVED` , 也就是根据key计算出来的`slot`不归7291端口管, 而是归7293端口管理,服务端返回`MOVED` 告诉客户端去7293端口操作. 

这个时候更换端口, 用`redis-cli -p 7293`  操作, 才会返回OK.或者用`./redis-cli -c -p port`的命令(c 代表`cluster`). 这样客户端需要连接两次. 

`jedis`等客户端会在本地维护一份`slot`-`node`的映射关系, 大部分的时候不需要重定向, 所以叫做`smart jedis`（需要客户端支持）

**问题:新增或下线了``Master* 节点, 数据怎么迁移(重新分配)**



#### 4.3.5 数据迁移

因为`key`与`slot` 的关系是永远不会变的, 当新增了节点的时候, 需要把原有的`slot`  分配给了新的节点负责, 并且把相关的数据迁移过来. 

新增加一个节点(新增一个7297)

```bash
redis-cli --cluster add-node 127.0.0.1:7291 127.0.0.1:7297

```



新增的节点也是没有哈希槽, 不能分布数据, 在原来的任意一个节点上运行. 

```bash
redis-cli --cluster reshard 127.0.0.1:7291

```

输入需要分配的哈希槽的数量(比如500), 和哈希槽的来源节点(可以输入`all` 或者id)

**问题:只有主节点可以写, 一个主节点挂了, 从节点怎么变成主节点? **



####  4.3.6 高可用和主从切换原理

当`slave` 发现自己的`master`变为`fail` 状态时, 便尝试进行`Failover`, 以期成为新的`master`. 由于挂掉的`master` 可能会有多个`slave`,从而存在多个`slave` 竞争成为`master`节点的过程, 其过程如下：

1. `slave` 发现自己的`master`变为`fail`
2. 将自己记录的集群`currentEpoch` 加1, 并广播`FAILOVER_AUTH_REQUEST` 信息
3. 其他节点收到该节点, 只有`master`响应, 判断请求者的合法性, 并发送`FAILOVER_AUTH_ACK`,对每一个`epoch`只发送一次`ack`
4. 尝试`failover` 的`slave`收集`FAILOVER_AUTH_ACK`
5. 超过半数后变成新`Master`
6. 广播`ping` 通知其他集群节点 



`redis cluster` 既能够实现主从的角色分配, 又能够实现主从切换, 相当于继承了`Replication` 和`sentinel`的功能. 



####  4.3.7   总结

#####  优势

1. 无中心架构
2. 数据按照`slot` 存储分布在多个节点, 节点间数据共享, 可动态调整数据分布
3. 可扩展性, 可线性扩展到1000个节点(官当推荐不超过1000个), 节点可动态添加或者删除. 
4. 高可用性, 部分节点不可用的时候, 集群扔可用. 通过增加`slave`做`standby` 数据副本, 能够实现故障自动`failover`, 节点之间通过`gossip`协议交换状态信息, 用投票机制完成`slave` 到`master`的角色提升. 
5. 降低运维成本, 提高系统的扩展性和可用性. 



#####  不足

1. client实现复杂, 驱动要求实现`smart client`, 缓存`slots mapping` 信息并及时更新, 提高了开发难度, 客户端的不成熟影响业务的稳定性. 
2. 节点会因为某些原因发生阻塞(阻塞时间大于`clutser-node-timeout`), 被判断下线, 这种`failover`是没有必要的. 
3. 数据通过异步复制, 不保证数据的强一致性. 
4. 多个业务系统使用同一套集群时, 无法根据统计区区分冷热数据, 资源隔离性较差, 容易出现互相影响的情况. 





